# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains data about bank customers. We seek to predict whether a customer will subscribe to a fixed term deposit.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was the voting ensemble.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The Scikit-learn pipeline used logistic regression. The hyperparameters being used with this model were regularization strength and maximum iterations.
The bank marketing data was used for this experiment as well as for the AutoML experiment.

**What are the benefits of the parameter sampler you chose?**
The random parameter sampler is a great sampling method because it is easy to use and accurately represents your sample.

**What are the benefits of the early stopping policy you chose?**
The BanditPolicy gives you greater control over which runs to terminate. The parameters I chose terminate runs
where the best metric is less than 91% of the best run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best model was the voting ensemble. A voting ensemble is the average of multiple machine learning models. The hyperparameters used are as follows:
  {
    random_state=0,
    reg_alpha=0,
    reg_lambda=1.0416666666666667,
    scale_pos_weight=1,
    seed=None,
    silent=None,
    subsample=0.9,
    tree_method='auto',
    verbose=-10,
    verbosity=0
  }

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The best HyperDrive run had an accuracy of 0.9102. The best AutoML run had an accuracy of 0.9179. The difference in accuracy was minimal at 0.77%. The data used for testing and training the models was the same; however, the subset of data used for training and testing was randomized. The models used were considerable different. The voting ensemble averages results from multiple machine learning models. Logistic regression is a single model used for regression style problems. The returned value is then categorized for use with classification in our case. The difference in accuracy is likely due to the ensemble learner being optimized for classification and the logistic regression learner being best suited for regression problems.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
We could allocate more resources to the compute cluster and allow for longer training sessions to try to improve the accuracy of our models.
We could use automated feature engineering to ensure we are selecting good features for the predictions we are attempting.
